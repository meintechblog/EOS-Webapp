"""signal backbone + retention + run snapshots

Revision ID: 20260220_0006
Revises: 20260219_0005
Create Date: 2026-02-20 00:15:00
"""

from __future__ import annotations

from datetime import datetime, timedelta, timezone
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "20260220_0006"
down_revision: Union[str, Sequence[str], None] = "20260219_0005"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def _month_start(value: datetime) -> datetime:
    return value.replace(day=1, hour=0, minute=0, second=0, microsecond=0)


def _month_partition_name(value: datetime) -> str:
    return f"signal_measurements_raw_{value.year:04d}{value.month:02d}"


def upgrade() -> None:
    op.create_table(
        "signal_catalog",
        sa.Column("id", sa.BigInteger(), sa.Identity(always=False), nullable=False),
        sa.Column("signal_key", sa.String(length=160), nullable=False),
        sa.Column("label", sa.String(length=160), nullable=False),
        sa.Column("value_type", sa.String(length=16), nullable=False),
        sa.Column("canonical_unit", sa.String(length=32), nullable=True),
        sa.Column(
            "tags_json",
            postgresql.JSONB(astext_type=sa.Text()),
            nullable=False,
            server_default=sa.text("'{}'::jsonb"),
        ),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False, server_default=sa.func.now()),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=False, server_default=sa.func.now()),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("signal_key", name="uq_signal_catalog_signal_key"),
        sa.CheckConstraint(
            "value_type IN ('number','string','bool','json')",
            name="ck_signal_catalog_value_type",
        ),
    )

    op.execute(
        """
        CREATE TABLE signal_measurements_raw (
            id BIGINT GENERATED BY DEFAULT AS IDENTITY NOT NULL,
            signal_id BIGINT NOT NULL REFERENCES signal_catalog(id),
            ts TIMESTAMPTZ NOT NULL,
            value_num DOUBLE PRECISION NULL,
            value_text TEXT NULL,
            value_bool BOOLEAN NULL,
            value_json JSONB NULL,
            quality_status VARCHAR(16) NOT NULL DEFAULT 'ok',
            source_type VARCHAR(32) NOT NULL,
            run_id BIGINT NULL REFERENCES eos_runs(id) ON DELETE SET NULL,
            mapping_id BIGINT NULL REFERENCES input_mappings(id) ON DELETE SET NULL,
            source_ref_id BIGINT NULL,
            ingested_at TIMESTAMPTZ NOT NULL DEFAULT now(),
            ingest_lag_ms INTEGER NULL,
            PRIMARY KEY (id, ts),
            CHECK (quality_status IN ('ok','derived','invalid','stale','estimated','missing')),
            CHECK (
                source_type IN (
                    'mqtt_input','fixed_input','eos_prediction','eos_plan','eos_solution','device_feedback','derived'
                )
            )
        ) PARTITION BY RANGE (ts)
        """
    )

    op.execute(
        """
        CREATE UNIQUE INDEX uq_signal_measurements_raw_dedupe
        ON signal_measurements_raw (
            signal_id,
            ts,
            source_type,
            COALESCE(run_id, 0),
            COALESCE(mapping_id, 0),
            COALESCE(source_ref_id, 0)
        )
        """
    )
    op.execute(
        "CREATE INDEX ix_signal_measurements_raw_signal_ts_desc ON signal_measurements_raw (signal_id, ts DESC)"
    )
    op.execute(
        "CREATE INDEX ix_signal_measurements_raw_source_ts_desc ON signal_measurements_raw (source_type, ts DESC)"
    )
    op.execute(
        "CREATE INDEX ix_signal_measurements_raw_run_ts_desc ON signal_measurements_raw (run_id, ts DESC)"
    )

    current_month = _month_start(datetime.now(timezone.utc))
    for month_offset in (-1, 0, 1, 2):
        start = _month_start(current_month + timedelta(days=month_offset * 31))
        start = _month_start(start)
        next_month = _month_start(start + timedelta(days=35))
        partition_name = _month_partition_name(start)
        op.execute(
            f"""
            CREATE TABLE IF NOT EXISTS {partition_name}
            PARTITION OF signal_measurements_raw
            FOR VALUES FROM ('{start.isoformat()}') TO ('{next_month.isoformat()}')
            """
        )

    op.execute(
        """
        CREATE TABLE IF NOT EXISTS signal_measurements_raw_default
        PARTITION OF signal_measurements_raw DEFAULT
        """
    )

    op.create_table(
        "signal_state_latest",
        sa.Column("signal_id", sa.BigInteger(), nullable=False),
        sa.Column("last_ts", sa.DateTime(timezone=True), nullable=True),
        sa.Column("last_value_num", sa.Float(), nullable=True),
        sa.Column("last_value_text", sa.Text(), nullable=True),
        sa.Column("last_value_bool", sa.Boolean(), nullable=True),
        sa.Column("last_value_json", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column("last_quality_status", sa.String(length=16), nullable=True),
        sa.Column("last_source_type", sa.String(length=32), nullable=True),
        sa.Column("last_run_id", sa.BigInteger(), nullable=True),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=False, server_default=sa.func.now()),
        sa.ForeignKeyConstraint(["signal_id"], ["signal_catalog.id"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(["last_run_id"], ["eos_runs.id"], ondelete="SET NULL"),
        sa.PrimaryKeyConstraint("signal_id"),
        sa.CheckConstraint(
            "last_quality_status IS NULL OR last_quality_status IN ('ok','derived','invalid','stale','estimated','missing')",
            name="ck_signal_state_latest_quality",
        ),
    )

    op.create_table(
        "eos_run_input_snapshots",
        sa.Column("id", sa.BigInteger(), sa.Identity(always=False), nullable=False),
        sa.Column("run_id", sa.BigInteger(), nullable=False),
        sa.Column("parameter_profile_id", sa.BigInteger(), nullable=True),
        sa.Column("parameter_revision_id", sa.BigInteger(), nullable=True),
        sa.Column("parameter_payload_json", postgresql.JSONB(astext_type=sa.Text()), nullable=False),
        sa.Column("mappings_snapshot_json", postgresql.JSONB(astext_type=sa.Text()), nullable=False),
        sa.Column("live_state_snapshot_json", postgresql.JSONB(astext_type=sa.Text()), nullable=False),
        sa.Column("runtime_config_snapshot_json", postgresql.JSONB(astext_type=sa.Text()), nullable=False),
        sa.Column("assembled_eos_input_json", postgresql.JSONB(astext_type=sa.Text()), nullable=False),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False, server_default=sa.func.now()),
        sa.ForeignKeyConstraint(["run_id"], ["eos_runs.id"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(["parameter_profile_id"], ["parameter_profiles.id"], ondelete="SET NULL"),
        sa.ForeignKeyConstraint(
            ["parameter_revision_id"],
            ["parameter_profile_revisions.id"],
            ondelete="SET NULL",
        ),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("run_id", name="uq_eos_run_input_snapshots_run_id"),
    )
    op.execute(
        "CREATE INDEX ix_eos_run_input_snapshots_created_desc ON eos_run_input_snapshots (created_at DESC)"
    )

    for table_name in ("signal_rollup_5m", "signal_rollup_1h", "signal_rollup_1d"):
        op.create_table(
            table_name,
            sa.Column("signal_id", sa.BigInteger(), nullable=False),
            sa.Column("bucket_start", sa.DateTime(timezone=True), nullable=False),
            sa.Column("min_num", sa.Float(), nullable=True),
            sa.Column("max_num", sa.Float(), nullable=True),
            sa.Column("avg_num", sa.Float(), nullable=True),
            sa.Column("sum_num", sa.Float(), nullable=True),
            sa.Column("count_num", sa.BigInteger(), nullable=False, server_default="0"),
            sa.Column("last_num", sa.Float(), nullable=True),
            sa.ForeignKeyConstraint(["signal_id"], ["signal_catalog.id"], ondelete="CASCADE"),
            sa.PrimaryKeyConstraint("signal_id", "bucket_start"),
        )
        op.execute(f"CREATE INDEX ix_{table_name}_bucket_desc ON {table_name} (bucket_start DESC)")

    op.create_table(
        "retention_job_runs",
        sa.Column("id", sa.BigInteger(), sa.Identity(always=False), nullable=False),
        sa.Column("job_name", sa.String(length=64), nullable=False),
        sa.Column("started_at", sa.DateTime(timezone=True), nullable=False),
        sa.Column("finished_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("status", sa.String(length=16), nullable=False),
        sa.Column("affected_rows", sa.BigInteger(), nullable=False, server_default="0"),
        sa.Column("details_json", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column("error_text", sa.Text(), nullable=True),
        sa.PrimaryKeyConstraint("id"),
        sa.CheckConstraint("job_name IN ('rollup','retention')", name="ck_retention_job_runs_name"),
        sa.CheckConstraint("status IN ('ok','error')", name="ck_retention_job_runs_status"),
    )
    op.execute("CREATE INDEX ix_retention_job_runs_started_desc ON retention_job_runs (started_at DESC)")

    op.add_column(
        "eos_mqtt_output_events",
        sa.Column(
            "output_kind",
            sa.String(length=16),
            nullable=False,
            server_default="unknown",
        ),
    )
    op.add_column(
        "eos_mqtt_output_events",
        sa.Column("resource_id", sa.String(length=128), nullable=True),
    )
    op.execute(
        "CREATE INDEX ix_eos_mqtt_output_events_kind_published_desc ON eos_mqtt_output_events (output_kind, published_at DESC)"
    )


def downgrade() -> None:
    op.execute("DROP INDEX IF EXISTS ix_eos_mqtt_output_events_kind_published_desc")
    op.drop_column("eos_mqtt_output_events", "resource_id")
    op.drop_column("eos_mqtt_output_events", "output_kind")

    op.execute("DROP INDEX IF EXISTS ix_retention_job_runs_started_desc")
    op.drop_table("retention_job_runs")

    for table_name in ("signal_rollup_1d", "signal_rollup_1h", "signal_rollup_5m"):
        op.execute(f"DROP INDEX IF EXISTS ix_{table_name}_bucket_desc")
        op.drop_table(table_name)

    op.execute("DROP INDEX IF EXISTS ix_eos_run_input_snapshots_created_desc")
    op.drop_table("eos_run_input_snapshots")

    op.drop_table("signal_state_latest")

    op.execute("DROP INDEX IF EXISTS ix_signal_measurements_raw_run_ts_desc")
    op.execute("DROP INDEX IF EXISTS ix_signal_measurements_raw_source_ts_desc")
    op.execute("DROP INDEX IF EXISTS ix_signal_measurements_raw_signal_ts_desc")
    op.execute("DROP INDEX IF EXISTS uq_signal_measurements_raw_dedupe")
    op.execute("DROP TABLE IF EXISTS signal_measurements_raw CASCADE")

    op.drop_table("signal_catalog")
